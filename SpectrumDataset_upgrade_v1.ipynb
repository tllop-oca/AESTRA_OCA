{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dba1a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from util import cubic_transform, torch_interp\n",
    "from scipy.interpolate import interp1d\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split, Dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea840d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrumDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Classe de chargement et prétraitement des spectres du RV-DataChallenge Plato.\n",
    "\n",
    "    Paramètres d'initialisation\n",
    "    ----------------------------\n",
    "    n_pixel : int\n",
    "        Nombre d’échantillons sur la grille de longueurs d’onde (L).  \n",
    "        Par défaut 2000.\n",
    "\n",
    "    lambda_min, lambda_max : float ou None\n",
    "        Bornes minimale et maximale du domaine spectral (en Å).  \n",
    "        Si None, on utilise les valeurs extraites du spectre d’origine.  \n",
    "        Par défaut 5000.0 – 5050.0 Å.\n",
    "\n",
    "    precision : {'torch.float32', 'torch.float64'}\n",
    "        Précision des tenseurs retournés par __getitem__ (utilisés lors de l'entraînement).\n",
    "\n",
    "    Attributs internes accessibles\n",
    "    ------------------------------\n",
    "    n_spec, n_pixel : int\n",
    "        Nombre de spectres et nombre de pixels par spectre.\n",
    "\n",
    "    all_specs_numpy : ndarray, shape (n_spec, n_pixel), dtype float64\n",
    "        Spectres bruts rééchantillonnés (ou non) sur la grille régulière.\n",
    "\n",
    "    wave_numpy : ndarray, shape (n_pixel,), dtype float64\n",
    "        Grille de longueurs d’onde.\n",
    "\n",
    "    template_spec_numpy : ndarray, shape (n_pixel,), dtype float64\n",
    "        Spectre « template » rééchantillonné.\n",
    "\n",
    "    all_specs_torch32/64 : Tensor, shape [n_spec, n_pixel]\n",
    "        Même données que all_specs_numpy, mais converties en float32 et float64.\n",
    "\n",
    "    wave_torch32/64 : Tensor, shape [n_pixel]\n",
    "        Même données que wave_numpy, converties en float32 et float64.\n",
    "\n",
    "    template_spec_torch32/64 : Tensor, shape [n_pixel]\n",
    "        Même données que template_spec_numpy, converties en float32 et float64.\n",
    "\n",
    "    c : float\n",
    "        Vitesse de la lumière (299 792 458 m/s).\n",
    "\n",
    "    c_tensor : Tensor, shape [1], dtype float64\n",
    "        Même valeur que c, en tenseur.\n",
    "\n",
    "    wavelength_step : float\n",
    "        Pas entre deux pixels consécutifs de la grille.\n",
    "\n",
    "    all_augmented_specs_* : tensors / ndarray\n",
    "        Versions « doppler-shiftées » agrégées (après appel à augment_data).\n",
    "\n",
    "    Méthodes clés\n",
    "    -------------\n",
    "    __len__()\n",
    "        Renvoie le nombre de spectres (n_spec).\n",
    "\n",
    "    __getitem__(index)\n",
    "        Retourne ((y_obs, y_aug, v_offset), index) selon self.precision.\n",
    "\n",
    "    doppler_shift_batch(...)\n",
    "        Applique un décalage doppler à un batch de spectres.\n",
    "\n",
    "    plot_spec(...)\n",
    "        Affiche un spectre (et optionnellement le template).\n",
    "\n",
    "    plot_doppler(...)\n",
    "        Affiche un spectre avant/après décalage doppler.\n",
    "\n",
    "    augment_data(...)\n",
    "        Génère automatiquement les spectres shiftés pour tout le dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_pixel = 2000, lambda_min=5000., lambda_max=5050., precision='torch.float32', device='cpu'):\n",
    "\n",
    "        # On charge le fichier Analyse_material, celui-ci contient le spectre template et la grille de longueur d'ondes\n",
    "        with open('STAR1134_HPN_Analyse_material.p', 'rb') as f:\n",
    "            analyse_material_data = pickle.load(f)\n",
    "\n",
    "        wave_numpy = analyse_material_data['wave'].to_numpy(dtype='float64') # -> dtype=float64\n",
    "        specs_numpy = np.load('STAR1134_HPN_flux_YVA.npy').astype('float64') # -> De taille (n_spec, n_pixel) / dtype=float64\n",
    "\n",
    "        # On peut choisir de crop ou non le spectre sur certaines zones de longueur d'ondes\n",
    "        if lambda_min is None:\n",
    "            lambda_min = wave_numpy.min()\n",
    "        if lambda_max is None:\n",
    "            lambda_max = wave_numpy.max()\n",
    "\n",
    "        # Mask pour crop\n",
    "        wave_mask = (wave_numpy >= lambda_min) & (wave_numpy <= lambda_max)\n",
    "\n",
    "        # On crop\n",
    "        wave_numpy = wave_numpy[wave_mask]\n",
    "        specs_numpy = specs_numpy[:, wave_mask]\n",
    "\n",
    "        # On récupère le spectre template que l'on crop aussi\n",
    "        template_spec_numpy = analyse_material_data['stellar_template'].to_numpy(dtype='float64') # -> dtype=float64\n",
    "        template_spec_numpy = template_spec_numpy[wave_mask]\n",
    "\n",
    "        # Réechantillonage des spectres sur une grille régulière si n_pixel est non nul ou non None\n",
    "        if n_pixel:\n",
    "            resampled_specs_numpy = []\n",
    "\n",
    "            wave_new_numpy = np.linspace(lambda_min, lambda_max, n_pixel, dtype='float64') # Grille régulière\n",
    "            for spec in specs_numpy:\n",
    "                spec_resampled = np.interp(wave_new_numpy, wave_numpy, spec)\n",
    "                resampled_specs_numpy.append(spec_resampled)\n",
    "            \n",
    "            resampled_specs_numpy = np.array(resampled_specs_numpy, dtype='float64')\n",
    "\n",
    "            self.all_specs_numpy = resampled_specs_numpy\n",
    "            self.wave_numpy = wave_new_numpy\n",
    "            template_spec_numpy = np.interp(wave_new_numpy, wave_numpy, template_spec_numpy) # Réechantillonage du spectre template aussi\n",
    "            self.template_spec_numpy = template_spec_numpy.astype('float64')\n",
    "\n",
    "        else:\n",
    "            # Sinon on garde les données initiales\n",
    "            self.all_specs_numpy = specs_numpy\n",
    "            self.wave_numpy = wave_numpy\n",
    "            self.template_spec_numpy = template_spec_numpy\n",
    "\n",
    "        self.device = device # Pour mettre ce qui est nécessaire sur GPU\n",
    "        \n",
    "        # Conversion en torch (dtype = float64)\n",
    "        self.all_specs_torch64 = torch.from_numpy(self.all_specs_numpy).to(self.device, non_blocking=True)\n",
    "        self.wave_torch64 = torch.from_numpy(self.wave_numpy).to(self.device, non_blocking=True)\n",
    "        self.template_spec_torch64 = torch.from_numpy(self.template_spec_numpy).to(self.device, non_blocking=True)\n",
    "\n",
    "        self.all_specs_torch32   = self.all_specs_torch64.to(torch.float32)\n",
    "        self.wave_torch32        = self.wave_torch64.to(torch.float32)\n",
    "        self.template_spec_torch32 = self.template_spec_torch64.to(torch.float32)\n",
    "\n",
    "        # Données supplémentaires utiles sur le dataset:\n",
    "        self.n_spec, self.n_pixel = self.all_specs_numpy.shape\n",
    "\n",
    "        self.c = 299_792_458.0\n",
    "        self.c_tensor = torch.tensor([299_792_458.0], dtype=torch.float64).to(self.device, non_blocking=True)\n",
    "\n",
    "        self.precision = precision\n",
    "\n",
    "        self.wavelength_step = (lambda_max - lambda_min) / (self.n_pixel - 1)\n",
    "\n",
    "        self.all_augmented_specs_torch64 = None\n",
    "        self.all_augmented_specs_torch32 = None\n",
    "        self.all_augmented_specs_numpy = None\n",
    "\n",
    "        self.all_voffsets64 = None\n",
    "        self.all_voffsets32 = None\n",
    "\n",
    "        self.augment_data(interp_method='torch_interp', batch_voffset=None)\n",
    "\n",
    "    # Renvoie la longueur du dataset (n_spec)\n",
    "    def __len__(self):\n",
    "        return self.n_spec\n",
    "    \n",
    "    # Affiche les infos du dataset avec print()\n",
    "    def __str__(self):\n",
    "        return (f\"-- Dataset de {self.n_spec} spectres de {self.n_pixel} pixels --\\n-- λmin = {self.wave_numpy.min()}, λmax = {self.wave_numpy.max()} --\\n-- Pas : {self.wavelength_step} λ --\")\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.precision == 'torch.float64':\n",
    "            batch_yobs = self.all_specs_torch64[index, :] # Renvoie un batch [B, n_pixel] dtype = torch.float32\n",
    "            batch_yaug = self.all_augmented_specs_torch64[index, :] # Renvoie un batch [B, n_pixel] dtype = torch.float32\n",
    "            batch_voffset = self.all_voffsets64[index]\n",
    "\n",
    "        else:\n",
    "            batch_yobs = self.all_specs_torch32[index, :] # Renvoie un batch [B, n_pixel] dtype = torch.float32\n",
    "            batch_yaug = self.all_augmented_specs_torch32[index, :] # Renvoie un batch [B, n_pixel] dtype = torch.float32\n",
    "            batch_voffset = self.all_voffsets32[index]\n",
    "            \n",
    "        return (batch_yobs, batch_yaug, batch_voffset), index\n",
    "\n",
    "    # Shift un batch de spectre avec un batch de vitesses d'offset données\n",
    "    def doppler_shift_batch(self, batch_yobs, batch_voffset, interp_method='torch_interp', out_dtype='torch.float32'):\n",
    "        \"\"\"\n",
    "            Simule un shift doppler pour un batch de spectre de dim [B, n_pixel] et un batch de vitesse d'offset de taille [B, 1]\n",
    "            Retourne un batch de spectres augmentés de taille [B, n_pixel]\n",
    "\n",
    "            On dispose de différentes d'interpolation : \n",
    "\n",
    "             * torch_interp -> Méthode linéaire parrallélisée -> gain de perf\n",
    "             * cubic_transform -> Le plus lent, Méthode utilisée par AESTRA\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Méthode parallélisée -> interpolation linéaire et valeurs de frontières fixées constantes\n",
    "        if interp_method == 'torch_interp':\n",
    "\n",
    "            # On travaille par batch\n",
    "            B = batch_yobs.shape[0]\n",
    "            \n",
    "            batch_wave = self.wave_torch64.unsqueeze(0) # [n_pixel] -> [1, n_pixel]\n",
    "            batch_wave = batch_wave.expand(B, -1) # [B, n_pixel]\n",
    "\n",
    "            if batch_voffset.dtype != 'torch.float64':\n",
    "                batch_voffset = batch_voffset.double()\n",
    "\n",
    "            if batch_yobs.dtype != 'torch.float64':\n",
    "                batch_yobs = batch_yobs.double()\n",
    "\n",
    "            batch_doppler_factor = torch.sqrt( (1 + batch_voffset/self.c_tensor) /  (1 - batch_voffset/self.c_tensor)) # [B, 1]\n",
    "            \n",
    "            # Pour vérifier si le facteur gamma ne vaut pas bêtement 1 pour cause de précision\n",
    "            # print('Valeur du facteur gamma : ', batch_doppler_factor[0].item()) \n",
    "\n",
    "            batch_wave_shifted = batch_wave * batch_doppler_factor # [B, n_pixel] * [B, 1] (Ressort en float64)\n",
    "\n",
    "            batch_yaug = torch_interp(batch_wave, batch_wave_shifted, batch_yobs) # [B, n_pixel] en float64\n",
    "\n",
    "            if out_dtype == 'torch.float64':\n",
    "                return batch_yaug.double() \n",
    "            else:\n",
    "                return batch_yaug.float() \n",
    "        \n",
    "        # Méthode utilisée dans AESTRA -> un peu plus rapide que scipy.interpolate.interp1d() sur de gros batch > 1000 mais sinon très lent\n",
    "        elif interp_method == 'cubic_transform':\n",
    "           \n",
    "            # Cubic transform attend 3 arguments : \n",
    "            #   - x_rest = tenseur de la grille de longueurs d'onde de taille    [n_pixel]\n",
    "            #   - y_rest = batch de spectres de taille                           [B, n_pixel]\n",
    "            #   - wave_shifted = batch des grilles de longueurs d'onde shiftée   [B, n_pixel] \n",
    "            \n",
    "\n",
    "            # On travaille par batch\n",
    "            B = batch_yobs.shape[0]\n",
    "\n",
    "            batch_wave = self.wave_torch64.unsqueeze(0) # [n_pixel] -> [1, n_pixel]\n",
    "            batch_wave = batch_wave.expand(B, -1) # [B, n_pixel]\n",
    "\n",
    "            if batch_voffset.dtype != 'torch.float64':\n",
    "                batch_voffset = batch_voffset.double()\n",
    "\n",
    "            if batch_yobs.dtype != 'torch.float64':\n",
    "                batch_yobs = batch_yobs.double()\n",
    "\n",
    "            batch_doppler_factor = torch.sqrt( (1 - batch_voffset/self.c_tensor) /  (1 + batch_voffset/self.c_tensor)) # [B, 1]\n",
    "            \n",
    "            batch_wave_shifted = batch_wave * batch_doppler_factor # [B, n_pixel]\n",
    "\n",
    "            batch_yaug = cubic_transform(\n",
    "                xrest=self.wave_torch64,\n",
    "                yrest=batch_yobs,\n",
    "                wave_shifted=batch_wave_shifted\n",
    "            )\n",
    "\n",
    "            if out_dtype == 'torch.float64':\n",
    "                return batch_yaug.double() \n",
    "            else:\n",
    "                return batch_yaug.float() \n",
    "            \n",
    "    # Plot un spec du dataset\n",
    "    def plot_spec(self, index = None, with_template=False):\n",
    "        \n",
    "        # Si on ne précise pas quel spectre plot on en prend un au hasard\n",
    "        if index is None:\n",
    "            index = np.random.randint(0, self.n_spec)\n",
    "        \n",
    "        spec_to_plot = self.all_specs_numpy[index, :]\n",
    "\n",
    "        plt.figure(figsize=(18, 6))\n",
    "        plt.title('Exemple de spectre')\n",
    "        if with_template:\n",
    "            plt.plot(self.wave_numpy, self.template_spec_numpy, linestyle='dashed', color='grey', label='Template')\n",
    "        plt.plot(self.wave_numpy, spec_to_plot, label=f'Spectre n°{index}')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(18, 6))\n",
    "        plt.title('(Zoom) Exemple de spectre')\n",
    "        if with_template:\n",
    "            plt.plot(self.wave_numpy, self.template_spec_numpy, linestyle='dashed', color='grey', label='Template')\n",
    "        plt.plot(self.wave_numpy, spec_to_plot, label=f'Spectre n°{index}')\n",
    "        plt.xlim(5012, 5013)\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "    # Plot un spec du dataset ainsi qu'un shift doppler pur de celui-ci\n",
    "    def plot_doppler(self, index = None, v_offset=950, plot_edges=False, interp_method='np.interp'):\n",
    "        if index is None:\n",
    "            index = np.random.randint(0, self.n_spec)\n",
    "        \n",
    "        spec_to_plot = self.all_specs_numpy[index, :]\n",
    "        \n",
    "        # Il faut le mettre sous forme de batch pour le rentrer dans la fonction doppler_shift_batch\n",
    "        batched_spec_to_plot = self.all_specs_torch64[index, :].unsqueeze(0) # Le unsqueeze rajoute la dimension du batch au début \n",
    "        batched_voffset = torch.tensor([v_offset]).unsqueeze(0) # Le unsqueeze rajoute la dimension du batch au début \n",
    "\n",
    "        batch_yaug = self.doppler_shift_batch(batched_spec_to_plot, batched_voffset, interp_method)\n",
    "\n",
    "        shifted_spec_to_plot = batch_yaug.cpu().squeeze().numpy()\n",
    "\n",
    "        plt.figure(figsize=(18, 6))\n",
    "        plt.title(f'Exemple de spectre shifté v={v_offset} m/s')\n",
    "        plt.plot(self.wave_numpy, spec_to_plot, label='Spectre original')\n",
    "        plt.plot(self.wave_numpy, shifted_spec_to_plot, label=f'Spectre Shifté voffset = {v_offset}')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "        # L'interpolation peut causer des problèmes au niveau des bords : plot_edges permet de visualiser les 10 premiers/derniers points\n",
    "        if plot_edges:\n",
    "            plt.figure(figsize=(18, 6))\n",
    "            plt.title(f'(Zoom) Bord Gauche v={v_offset} m/s')\n",
    "            plt.plot(self.wave_numpy, spec_to_plot, label='Spectre original', marker=\"x\")\n",
    "            plt.plot(self.wave_numpy, shifted_spec_to_plot, label=f'Spectre Shifté voffset = {v_offset}', marker=\"o\")\n",
    "            plt.xlim(self.wave_numpy.min() - 1 * self.wavelength_step, self.wave_numpy.min() + 10 * self.wavelength_step)\n",
    "            plt.legend()\n",
    "            plt.grid(alpha=0.5)\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(18, 6))\n",
    "            plt.title(f'(Zoom) Bord Droit v={v_offset} m/s')\n",
    "            plt.plot(self.wave_numpy, spec_to_plot, label='Spectre original', marker=\"x\")\n",
    "            plt.plot(self.wave_numpy, shifted_spec_to_plot, label=f'Spectre Shifté voffset = {v_offset}', marker=\"o\")\n",
    "            plt.xlim(self.wave_numpy.max() - 10 * self.wavelength_step, self.wave_numpy.max() + 1 * self.wavelength_step)\n",
    "            plt.legend()\n",
    "            plt.grid(alpha=0.5)\n",
    "            plt.show()\n",
    "    \n",
    "    def augment_data(self, interp_method='torch_interp', batch_voffset=None):\n",
    "        if batch_voffset is None:\n",
    "            batch_voffset = torch.rand(size=(self.n_spec, 1), dtype=torch.float64, device=self.device)* 6 - 3 # Distrib uniforme entre -3 et 3\n",
    "\n",
    "        self.all_augmented_specs_torch32 = self.doppler_shift_batch(\n",
    "            batch_yobs    = self.all_specs_torch64,  # On interpole sur tout les spectres\n",
    "            batch_voffset = batch_voffset,\n",
    "            interp_method = interp_method,\n",
    "            out_dtype     = 'torch.float32'\n",
    "            )\n",
    "        \n",
    "        self.all_augmented_specs_torch64 = self.doppler_shift_batch(\n",
    "            batch_yobs    = self.all_specs_torch64,  # On interpole sur tout les spectres\n",
    "            batch_voffset = batch_voffset,\n",
    "            interp_method = interp_method,\n",
    "            out_dtype     = 'torch.float64'\n",
    "            )\n",
    "        \n",
    "        self.all_augmented_specs_numpy = self.all_augmented_specs_torch64.cpu().numpy()\n",
    "\n",
    "        self.all_voffsets64 = batch_voffset\n",
    "        self.all_voffsets32 = batch_voffset.float()\n",
    "\n",
    "        print('Dataset augmenté !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19b32d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "        Réseau Multi-Perceptron classique\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_in, \n",
    "        n_out, \n",
    "        n_hidden=(16, 16, 16),\n",
    "        act=(nn.LeakyReLU(), nn.LeakyReLU(), nn.LeakyReLU(), nn.LeakyReLU()),\n",
    "        dropout=0\n",
    "    ):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        n_ = [n_in, *n_hidden, n_out]\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        for i in range(0, len(n_)-1):\n",
    "            layers.append(nn.Linear(in_features=n_[i], out_features=n_[i+1]))\n",
    "            layers.append(act[i])\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        conv_ksize,\n",
    "        conv_stride,\n",
    "        conv_padding,\n",
    "        maxpool_ksize,\n",
    "        maxpool_stride,\n",
    "        maxpool_padding,\n",
    "        maxpool_ceil_mode,\n",
    "        act=nn.LeakyReLU(),\n",
    "        dropout=0,\n",
    "        \n",
    "    ):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=out_channels, \n",
    "            kernel_size=conv_ksize, \n",
    "            stride=conv_stride, \n",
    "            padding=conv_padding\n",
    "        )\n",
    "        \n",
    "        self.instancenorm = nn.InstanceNorm1d(num_features=out_channels)\n",
    "        \n",
    "        self.activation = act\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Si on veut rajouter une couche maxpool (pas le cas du dernier convblock de spender)\n",
    "        if (maxpool_ksize is not None) and (maxpool_padding is not None) and (maxpool_stride is not None) and (maxpool_ceil_mode is not None):\n",
    "            self.maxpool = nn.MaxPool1d(\n",
    "                kernel_size=maxpool_ksize, \n",
    "                stride=maxpool_stride, \n",
    "                padding=maxpool_padding,\n",
    "                ceil_mode=maxpool_ceil_mode\n",
    "            )\n",
    "        else:\n",
    "            self.maxpool = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.instancenorm(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        if self.maxpool is not None:\n",
    "            x = self.maxpool(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class SPENDER(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "        * self.n_pixel_in              -> Nb de pixel du spectre en entrée           dtype=int\n",
    "        \n",
    "        * self.wave_block          -> tensor de taille [B, n_pixel_in]           dtype=float32\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, n_pixel_in):\n",
    "        super(SPENDER, self).__init__()\n",
    "        \n",
    "        # ---------- Encoder ----------\n",
    "        \n",
    "        # ConvBlock n°1\n",
    "        self.convblock1 = ConvBlock(\n",
    "            in_channels=1,\n",
    "            out_channels=128,\n",
    "            \n",
    "            conv_ksize=5,\n",
    "            conv_stride=1,\n",
    "            conv_padding=2,\n",
    "            \n",
    "            maxpool_ksize=5,\n",
    "            maxpool_stride=5,\n",
    "            maxpool_padding=0,\n",
    "            maxpool_ceil_mode=True,\n",
    "            \n",
    "            act=nn.PReLU(num_parameters=128),\n",
    "            dropout=0\n",
    "        )\n",
    "        \n",
    "        # ConvBlock n°2\n",
    "        self.convblock2 = ConvBlock(\n",
    "            in_channels=128,\n",
    "            out_channels=256,\n",
    "            \n",
    "            conv_ksize=11,\n",
    "            conv_stride=1,\n",
    "            conv_padding=5,\n",
    "            \n",
    "            maxpool_ksize=11,\n",
    "            maxpool_stride=11,\n",
    "            maxpool_padding=0,\n",
    "            maxpool_ceil_mode=True,\n",
    "            \n",
    "            act=nn.PReLU(num_parameters=256),\n",
    "            dropout=0\n",
    "        )\n",
    "        \n",
    "        # ConvBlock n°3\n",
    "        self.convblock3 = ConvBlock(\n",
    "            in_channels=256,\n",
    "            out_channels=512,\n",
    "            \n",
    "            conv_ksize=21,\n",
    "            conv_stride=1,\n",
    "            conv_padding=10,\n",
    "            \n",
    "            maxpool_ksize=None,\n",
    "            maxpool_stride=None,\n",
    "            maxpool_padding=None,\n",
    "            maxpool_ceil_mode=None,\n",
    "            \n",
    "            act=nn.PReLU(num_parameters=512),\n",
    "            dropout=0\n",
    "        )\n",
    "        \n",
    "        # Softmax du bloc d'attention\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # MLP pour convertir la sortie de l'attention block en vecteur de l'espace latent\n",
    "        self.latentMLP = MLP(\n",
    "            n_in=256,\n",
    "            n_out=3,\n",
    "            n_hidden=(128, 64, 32),\n",
    "            act=(nn.PReLU(128), nn.PReLU(64), nn.PReLU(32), nn.PReLU(3)),\n",
    "            dropout=0\n",
    "        )\n",
    "        \n",
    "        # ---------- Decoder ----------\n",
    "        self.decoder = MLP(\n",
    "            n_in=3,\n",
    "            n_out=n_pixel_in,\n",
    "            n_hidden=(64, 256, 1024),\n",
    "            act=(nn.PReLU(64), nn.PReLU(256), nn.PReLU(1024), nn.PReLU(n_pixel_in)),\n",
    "            dropout=0\n",
    "        )\n",
    "        \n",
    "        self.LSF = nn.Conv1d(1, 1, 5, bias=False, padding='same')\n",
    "\n",
    "        self.current_latent = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # Encoding\n",
    "        x = self.convblock1(x)\n",
    "        x = self.convblock2(x)\n",
    "        x = self.convblock3(x)\n",
    "        C = x.shape[1] // 2 # Nombre de canaux\n",
    "        h, k = torch.split(x, [C, C], dim=1) # On divise en deux\n",
    "        a = self.softmax(k)\n",
    "        e = torch.sum(h * a, dim=-1) # On somme selon les longueurs d'ondes -> sortie [B, C]\n",
    "        s = self.latentMLP(e) # Vecteur latent\n",
    "\n",
    "        self.current_latent = s\n",
    "        \n",
    "        # Decoding\n",
    "        x = self.decoder(s)\n",
    "\n",
    "        x = x.unsqueeze(1)\n",
    "                \n",
    "        # Convolve\n",
    "        x = self.LSF(x)\n",
    "\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd7becf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RVEstimator(nn.Module):\n",
    "    \"\"\"Some Information about RVEstimator\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_pixel_in,\n",
    "        dropout=0\n",
    "                 ):\n",
    "        super(RVEstimator, self).__init__()\n",
    "                \n",
    "        # ConvBlock n°1\n",
    "        self.convblock1 = ConvBlock(\n",
    "            in_channels=1,\n",
    "            out_channels=128,\n",
    "            \n",
    "            conv_ksize=5,\n",
    "            conv_stride=1,\n",
    "            conv_padding=2,\n",
    "            \n",
    "            maxpool_ksize=5,\n",
    "            maxpool_stride=5,\n",
    "            maxpool_padding=0,\n",
    "            maxpool_ceil_mode=False,\n",
    "            \n",
    "            act=nn.PReLU(num_parameters=128),\n",
    "            dropout=0\n",
    "        )\n",
    "        \n",
    "        # ConvBlock n°2\n",
    "        self.convblock2 = ConvBlock(\n",
    "            in_channels=128,\n",
    "            out_channels=64,\n",
    "            \n",
    "            conv_ksize=10,\n",
    "            conv_stride=1,\n",
    "            conv_padding=5,\n",
    "            \n",
    "            maxpool_ksize=10,\n",
    "            maxpool_stride=10,\n",
    "            maxpool_padding=0,\n",
    "            maxpool_ceil_mode=False,\n",
    "            \n",
    "            act=nn.PReLU(num_parameters=64),\n",
    "            dropout=0\n",
    "        )\n",
    "        \n",
    "        self.n_features_out = 64 * (n_pixel_in // 5) // 10\n",
    "        \n",
    "        # print(self.n_features_out)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.flatten = nn.Flatten() # Entrée de la taille [B, C, L] avec B = taille du batch, C = nb de canaux et L = segments wavelength\n",
    "        self.mlp = MLP(\n",
    "            n_in = self.n_features_out,\n",
    "            n_out = 1,\n",
    "            n_hidden=(128, 64, 32),\n",
    "            act=(nn.PReLU(128), nn.PReLU(64), nn.PReLU(32), nn.Identity()),\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.convblock1(x)\n",
    "        x = self.convblock2(x)\n",
    "        x = self.softmax(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a492df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader:\n",
    "    \"\"\"\n",
    "    DataLoader qui envoie automatiquement chaque batch sur le device.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dl = dataloader\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        for (batch_data, batch_idx) in self.dl:\n",
    "            # batch_data = (yobs, yaug, voffset)\n",
    "            yobs, yaug, voffset = batch_data\n",
    "            # envoyez chaque tenseur sur le device\n",
    "            yobs    = yobs.to(self.device, non_blocking=True)\n",
    "            yaug    = yaug.to(self.device, non_blocking=True)\n",
    "            voffset = voffset.to(self.device, non_blocking=True)\n",
    "            batch_idx = batch_idx.to(self.device, non_blocking=True)\n",
    "            yield ( (yobs, yaug, voffset), batch_idx )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6483d7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset augmenté !\n",
      "--- Epoch 1 ---\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m rvestimator\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     71\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (batch_data, batch_index) \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m     74\u001b[0m     (batch_yobs, batch_yaug, batch_voffset) \u001b[38;5;241m=\u001b[39m batch_data \u001b[38;5;66;03m# On déplie les valeurs contenu dnas le tuple batch data\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     current_B \u001b[38;5;241m=\u001b[39m batch_yobs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \n",
      "File \u001b[0;32m~/anaconda3/envs/sopagpu_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/sopagpu_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pin_memory_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/sopagpu_env/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py:72\u001b[0m, in \u001b[0;36mpin_memory\u001b[0;34m(data, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mSequence):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)([pin_memory(sample, device) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data])  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [pin_memory(sample, device) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/anaconda3/envs/sopagpu_env/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py:72\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mSequence):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)([\u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data])  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [pin_memory(sample, device) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/anaconda3/envs/sopagpu_env/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py:72\u001b[0m, in \u001b[0;36mpin_memory\u001b[0;34m(data, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mSequence):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)([pin_memory(sample, device) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data])  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [pin_memory(sample, device) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/anaconda3/envs/sopagpu_env/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py:72\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mSequence):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)([\u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data])  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [pin_memory(sample, device) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/anaconda3/envs/sopagpu_env/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py:57\u001b[0m, in \u001b[0;36mpin_memory\u001b[0;34m(data, device)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpin_memory\u001b[39m(data, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 57\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------- Hyperparamètres ----------- \n",
    "\n",
    "B = 64\n",
    "n_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "lambda_min = 5000\n",
    "lambda_max = 5050\n",
    "n_pixel = 2000 # = n_pixel\n",
    "precision = 'torch.float32'\n",
    "\n",
    "# LRV\n",
    "sigmav = 0.3\n",
    "\n",
    "# Lreg\n",
    "sigmay = 0.1\n",
    "k_reg = 0\n",
    "\n",
    "# ----------- Dataset -----------\n",
    "\n",
    "dataset = SpectrumDataset(n_pixel, lambda_min, lambda_max, precision=precision, device=device)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = int(0.2 * len(dataset))\n",
    "\n",
    "train_set, test_set = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=B, shuffle=False)\n",
    "test_dataloader  = DataLoader(test_set,  batch_size=B, shuffle=False)\n",
    "\n",
    "spec_template = dataset.template_spec_torch32\n",
    "n_pixel = dataset.n_pixel\n",
    "\n",
    "# ----------- Model -----------\n",
    "\n",
    "rvestimator = RVEstimator(n_pixel_in=n_pixel, dropout=0.)\n",
    "rvestimator_optimizer = torch.optim.Adam(\n",
    "    params=rvestimator.parameters(), \n",
    "    lr=learning_rate)\n",
    "\n",
    "spender = SPENDER(n_pixel_in=n_pixel)\n",
    "\n",
    "b_rest = nn.Parameter(torch.randn(size=(1, n_pixel)))\n",
    "\n",
    "aestra_optimizer = torch.optim.Adam(\n",
    "    params=list(rvestimator.parameters()) + list(spender.parameters()) + [b_rest],\n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(rvestimator_optimizer, max_lr=0.001, steps_per_epoch=len(train_dataloader), epochs=n_epochs)\n",
    "interp_method = 'torch_interp'\n",
    "\n",
    "# On met template_spec_torch sous forme de tenseur [1, n_pixel] pour profiter du broadcasting sans avoir à dupliquer\n",
    "b_obs = spec_template.unsqueeze(0)\n",
    "\n",
    "# ----------- GPU -----------\n",
    "\n",
    "rvestimator = rvestimator.to(device)\n",
    "spender     = spender.to(device)\n",
    "b_rest.data = b_rest.data.to(device)\n",
    "b_obs = b_obs.to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "for t in range(1, n_epochs + 1):\n",
    "    \n",
    "    print(f'--- Epoch {t} ---')\n",
    "    rvestimator.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    for (batch_data, batch_index) in train_dataloader:\n",
    "        \n",
    "        (batch_yobs, batch_yaug, batch_voffset) = batch_data # On déplie les valeurs contenu dnas le tuple batch data\n",
    "        \n",
    "        current_B = batch_yobs.shape[0] \n",
    "        \n",
    "        # On récupère la loss\n",
    "        batch_robs = (batch_yobs - b_obs)\n",
    "        batch_raug = (batch_yaug - b_obs)\n",
    "\n",
    "        batch_vobs_pred = rvestimator(batch_robs)\n",
    "        batch_vaug_pred = rvestimator(batch_raug)\n",
    "\n",
    "        batch_voffset_pred = (batch_vaug_pred - batch_vobs_pred)\n",
    "\n",
    "        L_RV = torch.nn.functional.mse_loss(batch_voffset_pred, batch_voffset)\n",
    "        train_loss += L_RV.item() * current_B\n",
    "\n",
    "        rvestimator_optimizer.zero_grad()\n",
    "        L_RV.backward()\n",
    "        rvestimator_optimizer.step()\n",
    "    \n",
    "    train_loss = train_loss / len(train_set)\n",
    "\n",
    "    print(f'-- Train Loss : {train_loss} --')\n",
    "\n",
    "    rvestimator.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    for (batch_data, batch_index) in test_dataloader:\n",
    "        \n",
    "        (batch_yobs, batch_yaug, batch_voffset) = batch_data # On déplie les valeurs contenu dnas le tuple batch data\n",
    "        \n",
    "        current_B = batch_yobs.shape[0] \n",
    "        \n",
    "        # On récupère la loss\n",
    "        batch_robs = (batch_yobs - b_obs)\n",
    "        batch_raug = (batch_yaug - b_obs)\n",
    "\n",
    "        batch_vobs_pred = rvestimator(batch_robs)\n",
    "        batch_vaug_pred = rvestimator(batch_raug)\n",
    "\n",
    "        batch_voffset_pred = (batch_vaug_pred - batch_vobs_pred)\n",
    "\n",
    "        L_RV = torch.nn.functional.mse_loss(batch_voffset_pred, batch_voffset)\n",
    "        test_loss += L_RV.item() * current_B\n",
    "\n",
    "    test_loss = test_loss / len(test_set)\n",
    "\n",
    "    print(f'-- Test Loss : {test_loss} --')\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Entraînement du RV Estimator terminé ! Durée = {end_time - start_time} secondes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45469abf",
   "metadata": {},
   "source": [
    "5.943692922592163 secondes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sopagpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
